<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
 "http://www.w3.org/TR/html4/loose.dtd">

<html>
<head>

<title>Transform-Rate, Fill-Rate</title>
<!--(Begin Meta)===================================================-->

<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="Originator" content="Pre-Expedition-RH" />
<meta name="Generator" content="Palimpsest by Emergent Game Technologies - http://emergent.net" />
<meta name="Last-Updated" content="%lastupdated%" />

<!--(End Meta)====================================================-->
    
<!--(Begin Links)===================================================-->
    
<link rel="stylesheet" href="../../shared/Emergent.css" type="text/css" />

<!--(End Links)=====================================================-->

</head>

<!--(Begin Body)===================================================-->
<body lang="EN-US">
<script src="../../shared/NavScript.js" language="JavaScript1.2"
type="text/javascript">
</script><script language="JavaScript1.2" type="text/javascript">
WriteNavLink(2);
</script>

<h1>Transform-Rate, Fill-Rate</h1>

<h3>Transform-Rate</h3>

<p>The transform-rate is the number of vertices a graphics card
can process in a given time period. When the number of vertices
to be transformed (moved, rotated, and lit) per time period
exceeds the graphics card's capabilities, an application is said
to be "transform limited."</p>

<p>The following table shows the maximum T&amp;L rate for various
PC graphics hardware:</p>

<!--(Table)=========================================================-->
<table>
    <thead>
        <tr>
            <th>Manufacturer &amp; Card</th>

            <th>T&amp;L - million vertex/sec</th>
        </tr>
    </thead>

    <tbody>
        <tr>
            <td>nVidia GeForce 3</td>

            <td>40</td>
        </tr>

        <tr>
            <td>nVidia GeForce 4 Ti</td>

            <td>136</td>
        </tr>

        <tr>
            <td>NVidia GeForce FX</td>

            <td>200</td>
        </tr>

        <tr>
            <td>nVidia GeForce 6800</td>

            <td>600</td>
        </tr>

        <tr>
            <td>ATI Radeon 8500</td>

            <td>75</td>
        </tr>

        <tr>
            <td>ATI Radeon 9700</td>

            <td>300</td>
        </tr>

        <tr>
            <td>ATI Radeon X800</td>

            <td>780</td>
        </tr>
    </tbody>
</table>

<p>These values are, of course, theoretical peaks and do not
represent real-world game situations. However, these numbers can
be useful in examining performance. If you wish to achieve 60 fps
on a GeForce 3, the absolute top amount of vertices you can
transform in a frame is 666,666. Let's say that every object you
render requires two passes. The theoretical top you could
transform is now halved to 333,333 vertices. Mind you, these
vertices all belong to one object and are untextured and flat
shaded, drawn as optimally as possible with absolutely nothing
else happening in the application. No interesting game could ever
hope to achieve this situation.</p>

<h3>Fill-Rate</h3>

<p>Not only is a graphics card limited in the number of vertices
it can transform, it is also limited in the number of pixels it
can write to the backbuffer per second. The backbuffer is the
portion of a graphics card's memory that is used as a scratch pad
while the final image is being assembled. When this process of
writing to the backbuffer exceeds the graphics card's capability,
an application is described as being "fill-rate limited."</p>

<p>The following chart shows the maximum fill rate for various PC
graphics hardware:</p>

<!--(Table)=========================================================-->
<table>
    <thead>
        <tr>
            <th>Manufacturer &amp; Card</th>

            <th>Pixel fillrate - million pixel/sec</th>
        </tr>
    </thead>

    <tbody>
        <tr>
            <td>nVidia GeForce 3</td>

            <td>800</td>
        </tr>

        <tr>
            <td>nVidia GeForce 4 Ti</td>

            <td>1200</td>
        </tr>

        <tr>
            <td>nVidia GeForce FX</td>

            <td>2000</td>
        </tr>

        <tr>
            <td>nVidia GeForce 6800</td>

            <td>6400</td>
        </tr>

        <tr>
            <td>ATI Radeon 8500</td>

            <td>1100</td>
        </tr>

        <tr>
            <td>ATI Radeon 9700</td>

            <td>2600</td>
        </tr>

        <tr>
            <td>ATI Radeon X800</td>

            <td>8300</td>
        </tr>
    </tbody>
</table>

<p>These values are, of course, theoretical peaks and do not
represent real-world game situations. However, these numbers can
be useful in examining performance. Let's see what we can do with
a GeForce 3 at a display resolution of 1024 by 768. We'll assume
for the moment that transformation and lighting comes for free
(which it never does). 1024 by 768 resolution is 786,432 pixels.
We'd like to run at 60 fps, so that involves rendering that 1024
by 768 image 60 times for a grand total of 47.19 million pixels.
Assuming each pixel is drawn more than once, the maximum number
of pixel writes we can do on each pixel is roughly 17. This, of
course, assumes that all operations that write a pixel cost the
same. Multitexturing and the complexity of pixel shaders quickly
lower this number. Overuse of complex pixel shaders can quickly
make an application fill rate limited.</p>

<!--kadov_tag{{<placeholder id="footer">}}-->

</body>

</html>
